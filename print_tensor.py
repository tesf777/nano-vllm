import os
import glob
import torch
import torch.distributed as dist
from transformers import Qwen3Config, AutoConfig
from nanovllm.models import qwen3


def init_torch_distributed():
    """åˆå§‹åŒ–torchåˆ†å¸ƒå¼ç¯å¢ƒ"""
    if not dist.is_initialized():
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ["RANK"] = "0"
        os.environ["WORLD_SIZE"] = "1"
        os.environ["MASTER_ADDR"] = "localhost"
        os.environ["MASTER_PORT"] = "12355"

        # åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„
        dist.init_process_group(
            backend="gloo",  # ä½¿ç”¨glooåç«¯ï¼Œé¿å…CUDAä¾èµ–
            rank=0,
            world_size=1
        )


def print_model_structure(model):
    """æ‰“å°æ¨¡å‹ç»“æ„ã€å‚æ•°ä¿¡æ¯å’Œtorch size"""
    print("\n" + "="*80)
    print("æ¨¡å‹ç»“æ„è¯¦ç»†ä¿¡æ¯")
    print("="*80)

    def print_module_info(module, prefix=""):
        """é€’å½’æ‰“å°æ¨¡å—ä¿¡æ¯"""
        for name, child in module.named_children():
            module_name = f"{prefix}.{name}" if prefix else name
            print(f"\nğŸ“¦ æ¨¡å—: {module_name}")
            print(f"   ç±»å‹: {type(child).__name__}")

            # æ‰“å°å‚æ•°ä¿¡æ¯
            param_count = 0
            for param_name, param in child.named_parameters():
                if param.data is not None:
                    param_count += 1
                    param_size = param.data.numel()
                    param_shape = tuple(param.data.shape)
                    print(f"   å‚æ•° {param_name}: {param_shape}, å…ƒç´ æ•°é‡: {param_size:,}, "
                          f"å¤§å°: {param_size * param.data.element_size() / 1024 / 1024:.2f} MB")

            print(f"   å‚æ•°æ€»æ•°: {param_count}")

            # å¦‚æœæœ‰å­æ¨¡å—ï¼Œé€’å½’æ‰“å°
            if len(list(child.children())) > 0:
                print_module_info(child, module_name)

    print_module_info(model)

    # æ‰“å°æ€»å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print("\n" + "="*80)
    print("æ¨¡å‹æ€»ç»Ÿè®¡")
    print("="*80)
    print(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB (å‡è®¾float32)")


def print_weight_details(model_path):
    """ç›´æ¥æ‰“å°safetensorsæ–‡ä»¶ä¸­çš„æƒé‡ä¿¡æ¯"""
    print("\n" + "="*80)
    print("æƒé‡æ–‡ä»¶è¯¦ç»†ä¿¡æ¯")
    print("="*80)

    from safetensors import safe_open

    safetensor_files = glob.glob(os.path.join(model_path, "*.safetensors"))
    print(f"æ‰¾åˆ° {len(safetensor_files)} ä¸ªsafetensorsæ–‡ä»¶:")

    weight_info = []
    total_size = 0

    for file_path in safetensor_files:
        file_name = os.path.basename(file_path)
        print(f"\nğŸ“„ æ–‡ä»¶: {file_name}")

        try:
            with safe_open(file_path, framework="pt", device="cpu") as f:
                weight_names = list(f.keys())
                print(f"   æƒé‡æ•°é‡: {len(weight_names)}")

                file_total_size = 0
                for weight_name in sorted(weight_names):
                    try:
                        tensor = f.get_tensor(weight_name)
                        tensor_size = tensor.numel()
                        tensor_shape = tuple(tensor.shape)
                        memory_size = tensor_size * tensor.element_size() / 1024 / 1024

                        # åªæ”¶é›†weight_nameå’Œtorchsizeä¿¡æ¯
                        weight_info.append({
                            'name': weight_name,
                            'shape': tensor_shape,
                            'size': tensor_size,
                            'memory_mb': memory_size
                        })

                        file_total_size += memory_size
                    except Exception as e:
                        print(f"   âŒ {weight_name}: è¯»å–é”™è¯¯ - {e}")

                print(f"   æ–‡ä»¶æ€»å¤§å°: {file_total_size:.2f} MB")
                total_size += file_total_size

        except Exception as e:
            print(f"   âŒ æ–‡ä»¶è¯»å–é”™è¯¯: {e}")

    print(f"\næ‰€æœ‰æƒé‡æ–‡ä»¶æ€»å¤§å°: {total_size:.2f} MB")
    return weight_info


def save_to_markdown(model_path, weight_info, total_params=None):
    """å°†æƒé‡ä¿¡æ¯ä¿å­˜åˆ°Markdownæ–‡ä»¶"""
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    model_name = os.path.basename(model_path.rstrip('/'))
    output_file = os.path.join(model_path, f"{model_name}_weights_info.md")

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"# {model_name} æ¨¡å‹æƒé‡ä¿¡æ¯\n\n")

        # åŸºæœ¬ä¿¡æ¯
        f.write("## åŸºæœ¬ä¿¡æ¯\n\n")
        f.write(f"- **æ¨¡å‹è·¯å¾„**: `{model_path}`\n")
        f.write(f"- **æƒé‡æ•°é‡**: {len(weight_info)}\n")
        if total_params:
            f.write(f"- **æ€»å‚æ•°æ•°é‡**: {total_params:,}\n")

        # è®¡ç®—æ€»å†…å­˜
        total_memory = sum(w['memory_mb'] for w in weight_info)
        f.write(f"- **æ€»å†…å­˜å ç”¨**: {total_memory:.2f} MB\n\n")

        # æƒé‡è¯¦æƒ…è¡¨æ ¼
        f.write("## æƒé‡è¯¦æƒ…\n\n")
        f.write("| æƒé‡åç§° | å½¢çŠ¶ | å…ƒç´ æ•°é‡ | å†…å­˜ (MB) |\n")
        f.write("|---------|------|----------|----------|\n")

        for weight in weight_info:
            f.write(f"| {weight['name']} | {weight['shape']} | {weight['size']:,} | {weight['memory_mb']:.2f} |\n")

        # æŒ‰å†…å­˜å¤§å°æ’åºçš„æƒé‡
        f.write("\n## æŒ‰å†…å­˜å¤§å°æ’åºçš„æƒé‡ (Top 20)\n\n")
        f.write("| æƒé‡åç§° | å½¢çŠ¶ | å†…å­˜ (MB) |\n")
        f.write("|---------|------|----------|\n")

        sorted_weights = sorted(weight_info, key=lambda x: x['memory_mb'], reverse=True)[:20]
        for weight in sorted_weights:
            f.write(f"| {weight['name']} | {weight['shape']} | {weight['memory_mb']:.2f} |\n")

    print(f"\nâœ… æƒé‡ä¿¡æ¯å·²ä¿å­˜åˆ°: {output_file}")
    return output_file


if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šä½¿ç”¨ load_model å‡½æ•°è¯»å– ./Qwen3-0.6B è·¯å¾„ä¸‹çš„æ¨¡å‹
    model_path = "./Qwen3-0.6B"

    # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    if os.path.exists(model_path):
        print(f"æ­£åœ¨åˆ†ææ¨¡å‹ä»è·¯å¾„: {model_path}")

        try:
            # è¯»å–æƒé‡ä¿¡æ¯å¹¶ä¿å­˜åˆ°Markdownæ–‡ä»¶
            weight_info = print_weight_details(model_path)
            total_params = None

            # å¯é€‰ï¼šåŠ è½½æ¨¡å‹è·å–æ€»å‚æ•°æ•°
            try:
                print("\nğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹ä»¥è·å–æ€»å‚æ•°æ•°...")
                # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
                init_torch_distributed()

                # åŠ è½½æ¨¡å‹é…ç½®
                config_path = os.path.join(model_path, "config.json")
                if os.path.exists(config_path):
                    config = AutoConfig.from_pretrained(model_path)
                    print(f"âœ… æˆåŠŸåŠ è½½é…ç½®")
                else:
                    # ä½¿ç”¨é»˜è®¤é…ç½®
                    config = Qwen3Config(
                        vocab_size=152064,
                        hidden_size=896,
                        intermediate_size=4864,
                        num_hidden_layers=24,
                        num_attention_heads=14,
                        num_key_value_heads=2,
                        max_position_embeddings=32768,
                        rms_norm_eps=1e-6,
                        hidden_act="silu"
                    )
                    print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")

                # åˆ›å»ºæ¨¡å‹å®ä¾‹
                model = qwen3.Qwen3ForCausalLM(config)
                print(f"âœ… æˆåŠŸåˆ›å»ºæ¨¡å‹å®ä¾‹")

                # è®¡ç®—æ€»å‚æ•°æ•°
                total_params = sum(p.numel() for p in model.parameters())
                print(f"âœ… æ¨¡å‹æ€»å‚æ•°æ•°: {total_params:,}")

            except Exception as e:
                print(f"âš ï¸  æ— æ³•åŠ è½½æ¨¡å‹è·å–æ€»å‚æ•°æ•°: {e}")
                print("   å°†ç»§ç»­ç”Ÿæˆæƒé‡ä¿¡æ¯æŠ¥å‘Šï¼ˆä¸åŒ…å«æ€»å‚æ•°æ•°ï¼‰")

            finally:
                # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
                if dist.is_initialized():
                    dist.destroy_process_group()

            # ä¿å­˜åˆ°Markdownæ–‡ä»¶
            save_to_markdown(model_path, weight_info, total_params)

        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

    else:
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        print(f"è¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")